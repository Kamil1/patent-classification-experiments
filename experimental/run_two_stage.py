#!/usr/bin/env python3
"""
Run Two-Stage Classification with Real Qwen Model

This script implements the actual two-stage approach:
1. Use existing DeBERTa results for high-confidence cases
2. Run Qwen reasoning on low-confidence cases
3. Combine results for final accuracy assessment
"""

import json
import os
import subprocess
import tempfile
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class TwoStageResult:
    """Result from two-stage classification."""
    predicted_class: int
    confidence: float
    method: str  # 'deberta' or 'qwen_reasoning'
    true_class: int
    is_correct: bool
    reasoning_explanation: Optional[str] = None
    original_deberta_pred: Optional[int] = None
    original_deberta_conf: Optional[float] = None

def load_deberta_results() -> Optional[Dict]:
    """Load existing DeBERTa results."""
    results_file = "./results/patent_classification_results_test_20251012_105849.json"
    
    try:
        with open(results_file, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Could not find DeBERTa results: {results_file}")
        return None

def create_reasoning_prompt(patent_text: str, deberta_pred: int, deberta_conf: float) -> str:
    """Create detailed reasoning prompt for Qwen."""
    
    class_labels = {
        0: "Human Necessities (food, agriculture, medicine, personal care)",
        1: "Performing Operations; Transporting (manufacturing, industrial processes)",
        2: "Chemistry; Metallurgy (chemical processes, materials science)",
        3: "Textiles; Paper (textile production, paper making)",  
        4: "Fixed Constructions (buildings, roads, construction)",
        5: "Mechanical Engineering; Lightning; Heating; Weapons; Blasting",
        6: "Physics (measurement, optics, nuclear physics, scientific instruments)",
        7: "Electricity (electrical circuits, power generation, electronics)",
        8: "General tagging of new or cross-sectional technology (emerging tech)"
    }
    
    prompt = f"""You are a patent classification expert. A DeBERTa AI model made a prediction but with low confidence. Please analyze this patent abstract step by step and determine the correct classification.

PATENT ABSTRACT:
{patent_text}

DEBERTA'S UNCERTAIN PREDICTION:
- Predicted Class: {deberta_pred} ({class_labels.get(deberta_pred, 'Unknown')})  
- Confidence: {deberta_conf:.3f} (LOW - needs expert analysis)

CLASSIFICATION CATEGORIES:
0: Human Necessities - food, agriculture, medicine, healthcare, personal care
1: Performing Operations; Transporting - manufacturing, industrial processes, transportation
2: Chemistry; Metallurgy - chemical processes, materials science, metallurgy, polymers
3: Textiles; Paper - textile production, paper making, fiber processing
4: Fixed Constructions - buildings, roads, bridges, construction, civil engineering
5: Mechanical Engineering; Lightning; Heating; Weapons; Blasting - mechanical systems, engines
6: Physics - measurement instruments, optics, nuclear physics, scientific analysis
7: Electricity - electrical circuits, power generation, electronics, telecommunications  
8: General tagging of new or cross-sectional technology - AI, nanotechnology, interdisciplinary

ANALYSIS INSTRUCTIONS:
1. Read the patent abstract carefully
2. Identify the main technical domain and key innovations
3. Determine the primary application area and industrial use
4. Consider what specific problem the invention solves
5. Match to the most appropriate classification category

Please provide your step-by-step reasoning, then conclude with:
FINAL_CLASSIFICATION: [number between 0-8]

Your analysis:"""
    
    return prompt

def run_qwen_reasoning(patent_texts: List[str], prompts: List[str]) -> List[str]:
    """Run REAL Qwen reasoning on multiple patents."""
    
    print(f"üß† Running REAL Qwen reasoning on {len(patent_texts)} low-confidence cases...")
    
    responses = []
    
    for i, prompt in enumerate(prompts):
        print(f"   Processing case {i+1}/{len(prompts)}...")
        
        # Create temporary file with the reasoning prompt
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(prompt)
            temp_file = f.name
        
        try:
            # Run REAL Qwen inference using the existing pipeline
            # We'll pass the prompt as the "patent text" to be classified
            cmd = [
                "python", "-c", f"""
import sys
sys.path.append('.')
from modal_client_flexible import FlexibleModalPatentClassifier

# Initialize Qwen client
client = FlexibleModalPatentClassifier(model_type="generative") 
client.initialize()

# Run classification with the reasoning prompt
result = client.classify_batch(['''{prompt}'''])
print(result[0]['raw_response'])
"""
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=180)
            
            if result.returncode == 0 and result.stdout.strip():
                response = result.stdout.strip()
                print(f"      ‚úÖ Got Qwen response: {len(response)} chars")
                responses.append(response)
            else:
                print(f"      ‚ùå Qwen failed: {result.stderr}")
                responses.append("FINAL_CLASSIFICATION: 0")  # Fallback
                
        except Exception as e:
            print(f"      ‚ùå Error running Qwen: {e}")
            responses.append("FINAL_CLASSIFICATION: 0")  # Fallback
            
        finally:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
    
    return responses

# Removed simulation function - using only real Qwen model

def parse_qwen_classification(response: str) -> Optional[int]:
    """Parse Qwen's response to extract the classification number."""
    
    import re
    
    patterns = [
        r"FINAL_CLASSIFICATION:\s*(\d+)",
        r"CLASSIFICATION:\s*(\d+)", 
        r"CLASS:\s*(\d+)",
        r"CATEGORY:\s*(\d+)"
    ]
    
    for pattern in patterns:
        match = re.search(pattern, response, re.IGNORECASE)
        if match:
            try:
                class_num = int(match.group(1))
                if 0 <= class_num <= 8:
                    return class_num
            except ValueError:
                continue
    
    return None

def run_two_stage_classification(confidence_threshold: float = 0.75) -> List[TwoStageResult]:
    """Run complete two-stage classification."""
    
    print("üöÄ RUNNING TWO-STAGE CLASSIFICATION")
    print("="*50)
    
    # Load DeBERTa results
    deberta_results = load_deberta_results()
    if not deberta_results:
        return []
    
    predictions = deberta_results['detailed_results']
    total_samples = len(predictions)
    
    print(f"üìä Analyzing {total_samples} DeBERTa predictions...")
    print(f"   Confidence threshold: {confidence_threshold}")
    
    # Separate high and low confidence cases
    high_confidence_cases = []
    low_confidence_cases = []
    reasoning_prompts = []
    
    for pred in predictions:
        confidence = pred.get('confidence', 0.5)
        
        if confidence >= confidence_threshold:
            high_confidence_cases.append(pred)
        else:
            low_confidence_cases.append(pred)
            # Create reasoning prompt for this case
            reasoning_prompt = create_reasoning_prompt(
                pred.get('text_preview', ''),
                pred.get('predicted_label', 0),
                confidence
            )
            reasoning_prompts.append(reasoning_prompt)
    
    print(f"   High confidence cases: {len(high_confidence_cases)}")
    print(f"   Low confidence cases (need reasoning): {len(low_confidence_cases)}")
    
    # Process results
    final_results = []
    
    # High confidence cases - keep DeBERTa predictions
    for pred in high_confidence_cases:
        result = TwoStageResult(
            predicted_class=pred.get('predicted_label', 0),
            confidence=pred.get('confidence', 0.5),
            method='deberta',
            true_class=pred.get('true_label', 0),
            is_correct=pred.get('is_correct', False)
        )
        final_results.append(result)
    
    # Low confidence cases - use Qwen reasoning
    if low_confidence_cases:
        print(f"\nüß† Running Qwen reasoning on {len(low_confidence_cases)} cases...")
        
        patent_texts = [pred.get('text_preview', '') for pred in low_confidence_cases]
        qwen_responses = run_qwen_reasoning(patent_texts, reasoning_prompts)
        
        for pred, response in zip(low_confidence_cases, qwen_responses):
            qwen_class = parse_qwen_classification(response)
            
            if qwen_class is not None:
                predicted_class = qwen_class
                confidence = 0.85  # High confidence in Qwen reasoning
                method = 'qwen_reasoning'
            else:
                # Fallback to DeBERTa if parsing failed
                predicted_class = pred.get('predicted_label', 0)  
                confidence = pred.get('confidence', 0.5) * 0.8
                method = 'deberta_fallback'
            
            true_class = pred.get('true_label', 0)
            is_correct = (predicted_class == true_class)
            
            result = TwoStageResult(
                predicted_class=predicted_class,
                confidence=confidence,
                method=method,
                true_class=true_class,
                is_correct=is_correct,
                reasoning_explanation=response,
                original_deberta_pred=pred.get('predicted_label', 0),
                original_deberta_conf=pred.get('confidence', 0.5)
            )
            final_results.append(result)
    
    return final_results

def evaluate_two_stage_results(results: List[TwoStageResult]) -> Dict:
    """Evaluate two-stage results."""
    
    if not results:
        return {}
    
    total_samples = len(results)
    total_correct = sum(1 for r in results if r.is_correct)
    
    # Break down by method
    deberta_results = [r for r in results if r.method == 'deberta']
    qwen_results = [r for r in results if r.method == 'qwen_reasoning']
    
    deberta_correct = sum(1 for r in deberta_results if r.is_correct)
    qwen_correct = sum(1 for r in qwen_results if r.is_correct) 
    
    # Calculate improvements (cases where Qwen was right but DeBERTa was wrong)
    improvements = []
    for r in qwen_results:
        if r.is_correct and r.original_deberta_pred != r.true_class:
            improvements.append(r)
    
    return {
        'total_samples': total_samples,
        'overall_accuracy': total_correct / total_samples,
        'deberta_cases': len(deberta_results),
        'deberta_accuracy': deberta_correct / len(deberta_results) if deberta_results else 0,
        'qwen_cases': len(qwen_results),
        'qwen_accuracy': qwen_correct / len(qwen_results) if qwen_results else 0,
        'improvements': len(improvements),
        'improvement_examples': improvements[:3]  # Top 3 examples
    }

def main():
    """Run the complete two-stage classification system."""
    
    # Run two-stage classification  
    results = run_two_stage_classification(confidence_threshold=0.75)
    
    if not results:
        print("‚ùå No results to analyze")
        return
    
    # Evaluate results
    evaluation = evaluate_two_stage_results(results)
    
    print(f"\nüéØ TWO-STAGE CLASSIFICATION RESULTS:")
    print(f"{'='*50}")
    print(f"Total Samples: {evaluation['total_samples']}")
    print(f"Overall Accuracy: {evaluation['overall_accuracy']:.3f}")
    print(f"")
    print(f"Method Breakdown:")
    print(f"  DeBERTa (high confidence): {evaluation['deberta_cases']} cases, {evaluation['deberta_accuracy']:.3f} accuracy")
    print(f"  Qwen Reasoning (low confidence): {evaluation['qwen_cases']} cases, {evaluation['qwen_accuracy']:.3f} accuracy")
    print(f"")
    print(f"Improvements: {evaluation['improvements']} cases where Qwen corrected DeBERTa")
    
    # Show improvement examples
    if evaluation['improvement_examples']:
        print(f"\n‚ú® IMPROVEMENT EXAMPLES:")
        for i, example in enumerate(evaluation['improvement_examples']):
            print(f"  {i+1}. DeBERTa: {example.original_deberta_pred} ‚Üí Qwen: {example.predicted_class} (True: {example.true_class})")
            if example.reasoning_explanation:
                print(f"     Reasoning: {example.reasoning_explanation[:100]}...")
    
    # Calculate improvement over original DeBERTa
    original_deberta_accuracy = 0.675  # From our previous results
    improvement = evaluation['overall_accuracy'] - original_deberta_accuracy
    
    print(f"\nüöÄ IMPROVEMENT ANALYSIS:")
    print(f"  Original DeBERTa: {original_deberta_accuracy:.3f}")
    print(f"  Two-Stage System: {evaluation['overall_accuracy']:.3f}")
    print(f"  Improvement: +{improvement:.3f} ({improvement*100:.1f}%)")
    
    print(f"\n‚úÖ TWO-STAGE CLASSIFICATION COMPLETE!")

if __name__ == "__main__":
    main()